---
title: "Attention in Stable Diffusion UNet"
date: 2025-06-13T20:06:31+08:00
draft: false
tags: ['Diffusion', 'SD 1.5', 'featured']
math: true
---

许多基于文生图模型的图像编辑方法，例如 prompt-to-prompt，ledits++ 等，都会利用 Stable Diffusion 的 UNet 里的注意力机制，来帮助定位编辑词在图像中的位置，从而实现文本引导的图像编辑。

这篇文章会彻底拆解 Stable Diffusion Unet 中，交叉注意力（Cross-Attention）的向量维度是如何计算和变化的。

为了让过程清晰，我们设定一个具体的场景：
-   **模型**: Stable Diffusion 1.5
-   **任务**: U-Net中的一个交叉注意力模块正在工作。
-   **批量大小 (Batch Size)**: 1（即一次只处理一张图片）。

### 准备阶段：两个输入源

在交叉注意力模块开始工作前，它会接收到两个输入：

1.  **图像侧的输入 (Image Hidden State)**
    -   这不是原始图片，而是图片在U-Net中经过一系列卷积层处理后得到的**特征图 (feature map)**。
    -   在U-Net的不同阶段，其维度不同。我们以一个典型的中间层为例，假设特征图维度是 `(1, 320, 32, 32)`。这代表 (批量大小, **通道数**, 高, 宽)。
    -   为了进行注意力计算，这个二维的特征图需要被“拉平”成一个序列。
    -   **拉平后维度**: `(1, 1024, 320)`
        -   `1`: 批量大小。
        -   `1024`: 序列长度 (32 * 32 = 1024)，代表图像被分成了1024个“小区域”或“像素块”。
        -   `320`: 每个小区域的特征向量维度。

2.  **文本侧的输入 (Text Embeddings)**
    -   这是你的文本提示（Prompt）经过CLIP文本编码器处理后得到的向量。
    -   **固定维度**: `(1, 77, 768)`
        -   `1`: 批量大小。
        -   `77`: 序列长度。SD 1.5的文本输入被固定为77个词元（Token）。
        -   `768`: 每个词元对应的语义向量维度。

**核心矛盾**：图像侧的向量维度是**320**，而文本侧是**768**。它们无法直接进行计算。交叉注意力的第一步就是解决这个维度不匹配的问题。

---

### 交叉注意力的详细步骤（维度之旅）

| 步骤 | 动作 | 输入维度 | 输出维度 | 解释 |
| :--- | :--- | :--- | :--- | :--- |
| **1** | **生成Query (Q) - 源自图像** | `(1, 1024, 320)` | `(1, 1024, 320)` | 图像特征通过一个线性层`W_Q`（权重矩阵尺寸为320x320）进行变换，生成Query。**维度不变，但内容变成了“查询”**。 |
| **2** | **生成Key (K) - 源自文本** | `(1, 77, 768)` | `(1, 77, 320)` | 文本特征通过一个线性层`W_K`（权重矩阵尺寸为**768x320**）进行变换。**维度从768降至320**，与图像侧对齐。 |
| **3** | **生成Value (V) - 源自文本** | `(1, 77, 768)` | `(1, 77, 320)` | 文本特征通过**另一个**线性层`W_V`（权重矩阵尺寸为**768x320**）进行变换。**维度从768降至320**。 |

**此时，我们有了三个维度对齐的向量：**
-   **Q (图像)**: `(1, 1024, 320)`
-   **K (文本)**: `(1, 77, 320)`
-   **V (文本)**: `(1, 77, 320)`

---

### 多头注意力：拆分与并行计算

为了让模型能从不同角度理解信息，SD 1.5使用了**多头注意力**（通常是8个头）。这意味着要把320维的向量拆成8份。

-   **每个头的维度**: 320 / 8 = **40**

| 步骤 | 动作 | 输入维度 | 输出维度 | 解释 |
| :--- | :--- | :--- | :--- | :--- |
| **4** | **拆分Q, K, V** | Q: `(1, 1024, 320)`<br>K: `(1, 77, 320)`<br>V: `(1, 77, 320)` | Q: `(1, 8, 1024, 40)`<br>K: `(1, 8, 77, 40)`<br>V: `(1, 8, 77, 40)` | 将每个向量的320维拆成8个40维的“头”。现在我们有8组独立的Q, K, V，可以并行计算。 |
| **5** | **计算注意力分数** | Q: `(1, 8, 1024, 40)`<br>K: `(1, 8, 77, 40)` | `(1, 8, 1024, 77)` | 将Q与K的转置 (`K_T`的维度是 `(1, 8, 40, 77)`)进行矩阵相乘。结果的**(1024, 77)**矩阵代表：1024个图像像素块，每个块对77个文本词元的关注度。 |
| **6** | **应用Softmax** | `(1, 8, 1024, 77)` | `(1, 8, 1024, 77)` | 对上一步的分数在最后一个维度（77）上进行Softmax归一化。现在对于每个图像块，它对所有77个文本词元的关注度之和为1。维度不变。 |
| **7** | **加权求和V** | 分数: `(1, 8, 1024, 77)`<br>V: `(1, 8, 77, 40)` | `(1, 8, 1024, 40)` | 将归一化后的注意力分数与V进行矩阵相乘。这一步是注意力的核心：**根据算出的“关注度”，对文本的Value信息进行加权求和**。 |
| **8** | **合并多头** | `(1, 8, 1024, 40)` | `(1, 1024, 320)` | 将8个头的结果“拼接”起来，恢复成原始的320维。 |
| **9** | **最终输出** | `(1, 1024, 320)` | `(1, 1024, 320)` | 合并后的向量再通过一个最终的线性层`W_O`（320x320）进行整合，得到最终注入了文本信息的图像特征。维度不变。 |

### 总结

整个过程就像一个精心设计的管道：

1.  **降维/对齐**：通过线性层（`W_K`, `W_V`），将高维的文本信息（768维）“投影”到U-Net正在工作的低维空间（320维），实现“语言”的统一。
2.  **拆分**：将统一后的向量拆成多个“头”，进行多角度分析。
3.  **查询与匹配**：图像（Q）向文本（K）发起查询，计算出每个图像块应该关注哪个文本词元。
4.  **信息提取**：根据匹配结果，从文本（V）中提取加权后的信息。
5.  **合并与输出**：将多角度提取的信息合并，更新原始的图像特征。

这个过程中，维度的变化是**有目的、受控制**的，其核心目的就是让**图像特征**能够有效地利用**文本特征**来指导自身的演化。